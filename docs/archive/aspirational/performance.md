# VexFS v2.0 Performance Reference

This comprehensive guide covers performance characteristics, optimization techniques, and benchmarking for VexFS v2.0.

## ðŸ“Š Performance Overview

VexFS v2.0 delivers exceptional performance through its dual-architecture design:

### Kernel Module Performance
- **Vector insertion**: >100,000 vectors/second
- **Search latency**: <1ms for 10-NN queries
- **Memory efficiency**: >90% cache utilization
- **Concurrent operations**: 1,000+ ops/second
- **Throughput**: 10GB/s+ on NVMe storage

### FUSE Implementation Performance
- **Vector insertion**: >50,000 vectors/second
- **Search latency**: <5ms for 10-NN queries
- **Memory efficiency**: >85% cache utilization
- **Cross-platform**: Linux, macOS compatibility

## ðŸ”§ Performance Factors

### Hardware Requirements

#### Optimal Configuration
```
CPU: 16+ cores, 3.0GHz+ (Intel Xeon or AMD EPYC)
Memory: 64GB+ DDR4-3200 or faster
Storage: NVMe SSD with 1M+ IOPS
Network: 25Gbps+ for distributed setups
```

#### Minimum Configuration
```
CPU: 4+ cores, 2.0GHz+
Memory: 16GB+ DDR4
Storage: SATA SSD with 50K+ IOPS
Network: 1Gbps for basic operations
```

### Software Configuration

#### Kernel Module Parameters
```bash
# High-performance configuration
sudo insmod vexfs_v2.ko \
    cache_size_mb=8192 \
    max_concurrent_ops=2000 \
    batch_size=50000 \
    worker_threads=16 \
    prefetch_enabled=1 \
    compression_level=1
```

#### System Tuning
```bash
# Optimize for VexFS workloads
echo 'vm.swappiness=1' >> /etc/sysctl.conf
echo 'vm.dirty_ratio=15' >> /etc/sysctl.conf
echo 'vm.dirty_background_ratio=5' >> /etc/sysctl.conf
echo 'kernel.sched_migration_cost_ns=5000000' >> /etc/sysctl.conf

# Apply settings
sysctl -p
```

## ðŸš€ Algorithm Performance

### HNSW (Hierarchical Navigable Small World)

#### Performance Characteristics
```
Search Complexity: O(log N)
Insert Complexity: O(log N)
Memory Usage: O(N * M * log N)
Recall: 95-99% (configurable)
```

#### Parameter Tuning

**High Throughput (Batch Operations)**
```python
collection = client.create_collection(
    name="high_throughput",
    dimension=384,
    algorithm="hnsw",
    parameters={
        "m": 8,                    # Lower connections for speed
        "ef_construction": 100,    # Faster construction
        "ef_search": 50,          # Faster search
        "max_m": 16,              # Limit max connections
        "ml": 1.0 / np.log(2.0)   # Standard level factor
    }
)

# Expected performance:
# - Insert: 150,000+ vectors/second
# - Search: 0.5ms average latency
# - Recall: ~92%
```

**Balanced Performance**
```python
collection = client.create_collection(
    name="balanced",
    dimension=384,
    algorithm="hnsw",
    parameters={
        "m": 16,                   # Standard connections
        "ef_construction": 200,    # Good construction quality
        "ef_search": 100,         # Good search quality
        "max_m": 32,              # Allow more connections
        "ml": 1.0 / np.log(2.0)   # Standard level factor
    }
)

# Expected performance:
# - Insert: 100,000+ vectors/second
# - Search: 1ms average latency
# - Recall: ~96%
```

**High Recall (Quality Focus)**
```python
collection = client.create_collection(
    name="high_recall",
    dimension=384,
    algorithm="hnsw",
    parameters={
        "m": 32,                   # More connections
        "ef_construction": 400,    # High construction quality
        "ef_search": 200,         # High search quality
        "max_m": 64,              # Maximum connections
        "ml": 1.0 / np.log(2.0)   # Standard level factor
    }
)

# Expected performance:
# - Insert: 50,000+ vectors/second
# - Search: 2ms average latency
# - Recall: ~99%
```

### LSH (Locality Sensitive Hashing)

#### Performance Characteristics
```
Search Complexity: O(1) average case
Insert Complexity: O(1)
Memory Usage: O(N + T * F)
Recall: 80-95% (configurable)
```

#### Parameter Tuning

**Memory Efficient**
```python
collection = client.create_collection(
    name="memory_efficient",
    dimension=384,
    algorithm="lsh",
    parameters={
        "num_tables": 10,         # Fewer tables
        "num_functions": 20,      # Fewer functions
        "bucket_size": 200,       # Larger buckets
        "projection_type": "random"
    }
)

# Expected performance:
# - Insert: 200,000+ vectors/second
# - Search: 0.3ms average latency
# - Memory: 50% less than HNSW
# - Recall: ~85%
```

**High Recall LSH**
```python
collection = client.create_collection(
    name="high_recall_lsh",
    dimension=384,
    algorithm="lsh",
    parameters={
        "num_tables": 30,         # More tables
        "num_functions": 40,      # More functions
        "bucket_size": 50,        # Smaller buckets
        "projection_type": "learned"
    }
)

# Expected performance:
# - Insert: 150,000+ vectors/second
# - Search: 0.8ms average latency
# - Memory: Similar to HNSW
# - Recall: ~93%
```

## ðŸ“ˆ Benchmarking

### Comprehensive Benchmark Suite

```python
import time
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import vexfs

class VexFSBenchmark:
    """Comprehensive VexFS performance benchmark"""
    
    def __init__(self, mount_path: str = '/mnt/vexfs'):
        self.client = vexfs.Client(mount_path)
        self.results = {}
    
    def generate_test_data(self, num_vectors: int, dimension: int) -> Tuple[np.ndarray, List[Dict]]:
        """Generate synthetic test data"""
        
        # Generate random vectors with some structure
        vectors = np.random.random((num_vectors, dimension)).astype(np.float32)
        
        # Normalize for cosine similarity
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        vectors = vectors / norms
        
        # Generate metadata
        metadata = [
            {
                "id": i,
                "category": f"category_{i % 10}",
                "subcategory": f"subcat_{i % 100}",
                "score": np.random.random(),
                "timestamp": f"2025-06-{(i % 30) + 1:02d}T10:00:00Z"
            }
            for i in range(num_vectors)
        ]
        
        return vectors, metadata
    
    def benchmark_insertion(self, collection, vectors: np.ndarray, metadata: List[Dict], 
                          batch_sizes: List[int] = [1, 100, 1000, 10000]) -> Dict:
        """Benchmark vector insertion performance"""
        
        results = {}
        
        for batch_size in batch_sizes:
            if batch_size > len(vectors):
                continue
                
            # Test batch insertion
            test_vectors = vectors[:batch_size]
            test_metadata = metadata[:batch_size]
            
            start_time = time.time()
            
            if batch_size == 1:
                # Single insertion
                for i in range(batch_size):
                    collection.insert(vector=test_vectors[i], metadata=test_metadata[i])
            else:
                # Batch insertion
                collection.insert_batch(
                    vectors=test_vectors,
                    metadata=test_metadata,
                    batch_size=min(1000, batch_size)
                )
            
            end_time = time.time()
            
            elapsed_time = end_time - start_time
            throughput = batch_size / elapsed_time
            
            results[batch_size] = {
                "elapsed_time": elapsed_time,
                "throughput": throughput,
                "vectors_per_second": throughput
            }
            
            print(f"Batch size {batch_size}: {throughput:.0f} vectors/second")
        
        return results
    
    def benchmark_search(self, collection, query_vectors: np.ndarray, 
                        k_values: List[int] = [1, 10, 100],
                        iterations: int = 100) -> Dict:
        """Benchmark search performance"""
        
        results = {}
        
        for k in k_values:
            latencies = []
            
            for i in range(min(iterations, len(query_vectors))):
                query_vector = query_vectors[i % len(query_vectors)]
                
                start_time = time.time()
                results_list = collection.search(vector=query_vector, limit=k)
                end_time = time.time()
                
                latency_ms = (end_time - start_time) * 1000
                latencies.append(latency_ms)
            
            results[k] = {
                "avg_latency_ms": np.mean(latencies),
                "p50_latency_ms": np.percentile(latencies, 50),
                "p95_latency_ms": np.percentile(latencies, 95),
                "p99_latency_ms": np.percentile(latencies, 99),
                "min_latency_ms": np.min(latencies),
                "max_latency_ms": np.max(latencies),
                "std_latency_ms": np.std(latencies)
            }
            
            print(f"k={k}: {results[k]['avg_latency_ms']:.2f}ms avg, "
                  f"{results[k]['p95_latency_ms']:.2f}ms p95")
        
        return results
    
    def benchmark_recall(self, collection, query_vectors: np.ndarray, 
                        ground_truth: List[List[int]], k: int = 10) -> Dict:
        """Benchmark search recall quality"""
        
        recalls = []
        
        for i, query_vector in enumerate(query_vectors):
            if i >= len(ground_truth):
                break
                
            # Get search results
            results = collection.search(vector=query_vector, limit=k)
            result_ids = [r.metadata["id"] for r in results]
            
            # Calculate recall
            true_neighbors = set(ground_truth[i][:k])
            found_neighbors = set(result_ids)
            
            recall = len(true_neighbors.intersection(found_neighbors)) / len(true_neighbors)
            recalls.append(recall)
        
        return {
            "avg_recall": np.mean(recalls),
            "min_recall": np.min(recalls),
            "max_recall": np.max(recalls),
            "std_recall": np.std(recalls)
        }
    
    def benchmark_memory_usage(self, collection) -> Dict:
        """Benchmark memory usage"""
        
        stats = collection.get_stats()
        system_stats = self.client.get_stats()
        
        return {
            "collection_memory_mb": stats.index_size_mb,
            "system_memory_mb": system_stats.memory_usage_mb,
            "vectors_per_mb": stats.vector_count / max(stats.index_size_mb, 1),
            "cache_hit_rate": system_stats.cache_hits / max(system_stats.cache_hits + system_stats.cache_misses, 1)
        }
    
    def run_comprehensive_benchmark(self, algorithms: List[str] = ["hnsw", "lsh"],
                                  dataset_sizes: List[int] = [1000, 10000, 100000],
                                  dimension: int = 384) -> Dict:
        """Run comprehensive benchmark across algorithms and dataset sizes"""
        
        results = {}
        
        for algorithm in algorithms:
            results[algorithm] = {}
            
            for size in dataset_sizes:
                print(f"\nBenchmarking {algorithm} with {size} vectors...")
                
                # Generate test data
                vectors, metadata = self.generate_test_data(size, dimension)
                query_vectors = vectors[:100]  # Use first 100 as queries
                
                # Create collection
                collection_name = f"benchmark_{algorithm}_{size}"
                
                if algorithm == "hnsw":
                    collection = self.client.create_collection(
                        name=collection_name,
                        dimension=dimension,
                        algorithm="hnsw",
                        parameters={"m": 16, "ef_construction": 200, "ef_search": 100}
                    )
                else:  # LSH
                    collection = self.client.create_collection(
                        name=collection_name,
                        dimension=dimension,
                        algorithm="lsh",
                        parameters={"num_tables": 20, "num_functions": 30}
                    )
                
                # Benchmark insertion
                print("  Benchmarking insertion...")
                insertion_results = self.benchmark_insertion(collection, vectors, metadata)
                
                # Benchmark search
                print("  Benchmarking search...")
                search_results = self.benchmark_search(collection, query_vectors)
                
                # Benchmark memory
                print("  Benchmarking memory...")
                memory_results = self.benchmark_memory_usage(collection)
                
                results[algorithm][size] = {
                    "insertion": insertion_results,
                    "search": search_results,
                    "memory": memory_results
                }
                
                # Cleanup
                self.client.delete_collection(collection_name)
        
        return results
    
    def generate_report(self, results: Dict) -> str:
        """Generate performance report"""
        
        report = "# VexFS v2.0 Performance Benchmark Report\n\n"
        
        for algorithm in results:
            report += f"## {algorithm.upper()} Algorithm\n\n"
            
            for size in results[algorithm]:
                data = results[algorithm][size]
                report += f"### Dataset Size: {size:,} vectors\n\n"
                
                # Insertion performance
                report += "**Insertion Performance:**\n"
                for batch_size, metrics in data["insertion"].items():
                    report += f"- Batch size {batch_size}: {metrics['throughput']:.0f} vectors/second\n"
                
                # Search performance
                report += "\n**Search Performance:**\n"
                for k, metrics in data["search"].items():
                    report += f"- k={k}: {metrics['avg_latency_ms']:.2f}ms avg, {metrics['p95_latency_ms']:.2f}ms p95\n"
                
                # Memory usage
                report += "\n**Memory Usage:**\n"
                report += f"- Index size: {data['memory']['collection_memory_mb']:.1f}MB\n"
                report += f"- Vectors per MB: {data['memory']['vectors_per_mb']:.0f}\n"
                report += f"- Cache hit rate: {data['memory']['cache_hit_rate']:.2%}\n\n"
        
        return report

# Example usage
benchmark = VexFSBenchmark()

# Run quick benchmark
results = benchmark.run_comprehensive_benchmark(
    algorithms=["hnsw", "lsh"],
    dataset_sizes=[1000, 10000],
    dimension=384
)

# Generate report
report = benchmark.generate_report(results)
print(report)
```

### Real-World Performance Tests

```python
def test_real_world_scenarios():
    """Test performance in real-world scenarios"""
    
    client = vexfs.Client('/mnt/vexfs')
    
    # Scenario 1: Document Search (384-dim embeddings)
    print("Scenario 1: Document Search")
    doc_collection = client.create_collection(
        name="documents",
        dimension=384,
        algorithm="hnsw",
        parameters={"m": 16, "ef_construction": 200}
    )
    
    # Insert 100K document embeddings
    vectors = np.random.random((100000, 384)).astype(np.float32)
    metadata = [{"doc_id": i, "category": f"cat_{i%20}"} for i in range(100000)]
    
    start_time = time.time()
    doc_collection.insert_batch(vectors, metadata, batch_size=5000)
    insert_time = time.time() - start_time
    
    print(f"  Inserted 100K documents in {insert_time:.1f}s ({100000/insert_time:.0f} docs/sec)")
    
    # Test search performance
    query_times = []
    for _ in range(100):
        query = np.random.random(384).astype(np.float32)
        start_time = time.time()
        results = doc_collection.search(query, limit=10)
        query_times.append((time.time() - start_time) * 1000)
    
    print(f"  Search latency: {np.mean(query_times):.2f}ms avg, {np.percentile(query_times, 95):.2f}ms p95")
    
    # Scenario 2: Image Search (2048-dim embeddings)
    print("\nScenario 2: Image Search")
    image_collection = client.create_collection(
        name="images",
        dimension=2048,
        algorithm="hnsw",
        parameters={"m": 32, "ef_construction": 400}
    )
    
    # Insert 50K image embeddings
    vectors = np.random.random((50000, 2048)).astype(np.float32)
    metadata = [{"image_id": i, "category": f"img_cat_{i%10}"} for i in range(50000)]
    
    start_time = time.time()
    image_collection.insert_batch(vectors, metadata, batch_size=2000)
    insert_time = time.time() - start_time
    
    print(f"  Inserted 50K images in {insert_time:.1f}s ({50000/insert_time:.0f} images/sec)")
    
    # Test search performance
    query_times = []
    for _ in range(50):
        query = np.random.random(2048).astype(np.float32)
        start_time = time.time()
        results = image_collection.search(query, limit=20)
        query_times.append((time.time() - start_time) * 1000)
    
    print(f"  Search latency: {np.mean(query_times):.2f}ms avg, {np.percentile(query_times, 95):.2f}ms p95")
    
    # Scenario 3: Real-time Recommendations
    print("\nScenario 3: Real-time Recommendations")
    rec_collection = client.create_collection(
        name="recommendations",
        dimension=128,
        algorithm="lsh",
        parameters={"num_tables": 15, "num_functions": 25}
    )
    
    # Insert 1M user/item embeddings
    vectors = np.random.random((1000000, 128)).astype(np.float32)
    metadata = [{"item_id": i, "category": f"rec_cat_{i%50}"} for i in range(1000000)]
    
    start_time = time.time()
    rec_collection.insert_batch(vectors, metadata, batch_size=10000)
    insert_time = time.time() - start_time
    
    print(f"  Inserted 1M items in {insert_time:.1f}s ({1000000/insert_time:.0f} items/sec)")
    
    # Test real-time search performance
    query_times = []
    for _ in range(1000):
        query = np.random.random(128).astype(np.float32)
        start_time = time.time()
        results = rec_collection.search(query, limit=50)
        query_times.append((time.time() - start_time) * 1000)
    
    print(f"  Search latency: {np.mean(query_times):.2f}ms avg, {np.percentile(query_times, 95):.2f}ms p95")
    print(f"  Throughput: {1000/np.mean(query_times):.0f} queries/second")

test_real_world_scenarios()
```

## ðŸ”§ Optimization Techniques

### Memory Optimization

```python
def optimize_memory_usage():
    """Optimize VexFS memory usage"""
    
    # 1. Configure appropriate cache sizes
    client = vexfs.Client('/mnt/vexfs')
    
    # Calculate optimal cache size (rule of thumb: 10-20% of available RAM)
    import psutil
    available_memory_gb = psutil.virtual_memory().available / (1024**3)
    optimal_cache_gb = min(available_memory_gb * 0.15, 16)  # Cap at 16GB
    
    collection = client.create_collection(
        name="memory_optimized",
        dimension=384,
        algorithm="hnsw",
        cache_size=f"{optimal_cache_gb:.1f}GB"
    )
    
    # 2. Use appropriate data types
    # Use float32 instead of float64 for 50% memory savings
    vectors = np.random.random((100000, 384)).astype(np.float32)  # Not float64
    
    # 3. Batch operations efficiently
    # Larger batches = better memory utilization
    optimal_batch_size = min(10000, len(vectors) // 10)
    collection.insert_batch(vectors, batch_size=optimal_batch_size)
    
    # 4. Monitor memory usage
    stats = collection.get_stats()
    print(f"Memory efficiency: {stats.vector_count / stats.index_size_mb:.0f} vectors/MB")

def optimize_cpu_usage():
    """Optimize CPU utilization"""
    
    # 1. Configure worker threads based on CPU cores
    import os
    cpu_cores = os.cpu_count()
    optimal_workers = min(cpu_cores, 16)  # Cap at 16 workers
    
    # 2. Use parallel batch operations
    collection.configure_parallelism(
        insert_workers=optimal_workers,
        search_workers=optimal_workers // 2,  # Search is more CPU intensive
        io_workers=4
    )
    
    # 3. Optimize search parameters for CPU cache
    # Smaller ef_search values = better CPU cache utilization
    collection.configure_search(ef_search=50)  # Start conservative
    
    # 4. Use SIMD-optimized distance calculations
    collection.enable_simd_optimization(True)

def optimize_storage_io():
    """Optimize storage I/O performance"""
    
    # 1. Configure appropriate I/O patterns
    collection.configure_io(
        read_ahead_kb=1024,      # Optimize for sequential reads
        write_batch_size=64,     # Batch writes for efficiency
        sync_frequency=1000,     # Sync every 1000 operations
        use_direct_io=True       # Bypass page cache for large datasets
    )
    
    # 2. Monitor I/O patterns
    io_stats = collection.get_io_stats()
    print(f"Read IOPS: {io_stats.read_iops}")
    print(f"Write IOPS: {io_stats.write_iops}")
    print(f"Average latency: {io_stats.avg_latency_ms:.2f}ms")
```

### Query Optimization

```python
def optimize_search_queries():
    """Optimize search query performance"""
    
    client = vexfs.Client('/mnt/vexfs')
    collection = client.get_collection("optimized_search")
    
    # 1. Use appropriate search parameters
    def adaptive_ef_search(k: int, collection_size: int) -> int:
        """Calculate optimal ef_search based on k and collection size"""
        base_ef = max(k * 2, 50)  # At least 2x k, minimum 50
        
        if collection_size < 10000:
            return base_ef
        elif collection_size < 100000:
            return base_ef * 2
        else:
            return base_ef * 3
    
    # 2. Optimize filter queries
    def optimize_filters(filter_dict: dict) -> dict:
        """Optimize filter queries for better performance"""
        
        # Move most selective filters first
        if "$and" in filter_dict:
            conditions = filter_dict["$and"]
            # Sort by estimated selectivity (smaller result sets first)
            conditions.sort(key=lambda x: estimate_selectivity(x))
            filter_dict["$and"] = conditions
        
        return filter_dict
    
    def estimate_selectivity(condition: dict) -> float:
        """Estimate filter selectivity (0 = most selective, 1 = least selective)"""
        if "category" in condition:
            return 0.1  # Categories are usually selective
        elif "timestamp" in condition:
            return 0.3  # Timestamps are moderately selective
        else:
            return 0.5  # Default selectivity
    
    # 3. Use query caching for repeated searches
    from functools import lru_cache
    
    @lru_cache(maxsize=1000)
    def cached_search(query_hash: str, k: int, filter_str: str):
        """Cache search results for repeated queries"""
        # Convert back from hash/string to actual objects
        query_vector = deserialize_vector(query_hash)
        filter_dict = json.loads(filter_str) if filter_str else None
        
        return collection.search(query_vector, limit=k, filter=filter_dict)
    
    # 4. Batch similar queries
    def batch_similar_queries(queries: list, k: int = 10) -> list:
        """Batch similar queries for better performance"""
        
        # Group queries by similarity
        query_groups = group_similar_queries(queries, threshold=0.9)
        
        results = []
        for group in query_groups:
            # Use representative query for the group
            representative = np.mean([q["vector"] for q in group], axis=0)
            group_results = collection.search(representative, limit=k * 2)
            
            # Distribute results to individual queries
            for query in group:
                query_results = rerank_for_query(group_results, query["vector"], k)
                results.append(query_results)
        
        return results

def monitor_query_performance():
    """Monitor and analyze query performance"""
    
    class QueryProfiler:
        def __init__(self):
            self.query_log = []
        
        def profile_query(self, query_func, *args, **kwargs):
            """Profile a query execution"""
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss
            
            result = query_func(*args, **kwargs)
            
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss
            
            profile_data = {
                "execution_time": end_time - start_time,
                "memory_delta": end_memory - start_memory,
                "result_count": len(result) if hasattr(result, '__len__') else 1,
                "timestamp": time.time()
            }
            
            self.query_log.append(profile_data)
            return result
        
        def get_performance_summary(self):
            """Get performance summary"""
            if not self.query_log:
                return {}
            
            times = [log["execution_time"] for log in self.query_log]
            memory_deltas = [log["memory_delta"] for log in self.query_log]
            
            return {
                "avg_execution_time": np.mean(times),
                "p95_execution_time": np.percentile(times, 95),
                "avg_memory_delta": np.mean(memory_deltas),
                "total_queries": len(self.query_log)
            }
    
    # Usage example
    profiler = QueryProfiler()
    
    # Profile searches
    for _ in range(100):
        query_vector = np.random.random(384).astype(np.float32)
        results = profiler.profile_query(
            collection.search,
            query_vector,
            limit=10
        )
    
    summary = profiler.get_performance_summary()
    print(f"Average query time: {summary['avg_execution_time']*1000:.2f}ms")
    print(f"P95 query time: {summary['p95_execution_time']*1000:.2f}ms")
```

## ðŸ“Š Performance Monitoring

### Real-time Monitoring

```python
import threading
import time
from collections import deque

class VexFSMonitor:
    """Real-time VexFS performance monitor"""
    
    def __init__(self, client: vexfs.Client, collection_name: str):
        self.client = client
        self.collection_name = collection_name
        self.collection = client.get_collection(collection_name)
        
        self.metrics_history = deque(maxlen=1000)  # Keep last 1000 measurements
        self.monitoring = False
        self.monitor_thread = None
    
    def start_monitoring(self, interval: float = 1.0):
        """Start real-time monitoring"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            args=(interval,),
            daemon=True
        )
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """Stop monitoring"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
    
    def _monitor_loop(self, interval: float):
        """Main monitoring loop"""
        while self.monitoring:
            try:
                # Collect metrics
                collection_stats = self.collection.get_stats()
                system_stats = self.client.get_stats()
                
                # System metrics
                import psutil
                cpu_percent = psutil.cpu_percent()
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage('/')
                
                metrics = {
                    "timestamp": time.time(),
                    "collection": {
                        "vector_count": collection_stats.vector_count,
                        "search_count": collection_stats.search_count,
                        "insert_count": collection_stats.insert_count,
                        "index_size_mb": collection_stats.index_size_mb
                    },
                    "system": {
                        "cache_hit_rate": system_stats.cache_hits / max(system_stats.cache_hits + system_stats.cache_misses, 1),
                        "avg_search_time_ms": system_stats.avg_search_time_ms,
                        "memory_usage_mb": system_stats.memory_usage_mb
                    },
                    "host": {
                        "cpu_percent": cpu_percent,
                        "memory_percent": memory.percent,
                        "