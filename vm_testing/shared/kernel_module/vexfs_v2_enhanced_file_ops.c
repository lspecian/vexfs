/*
 * VexFS v2.0 Enhanced File Operations Implementation
 * 
 * This file implements vector-optimized file operations with SIMD acceleration,
 * memory mapping, and intelligent readahead strategies for optimal vector
 * database performance.
 */

#include <linux/module.h>
#include <linux/fs.h>
#include <linux/mm.h>
#include <linux/mman.h>
#include <linux/uaccess.h>
#include <linux/prefetch.h>
#include <linux/highmem.h>
#include <linux/pagemap.h>
#include <linux/writeback.h>
#include <linux/slab.h>
#include <linux/vmalloc.h>
#include <linux/numa.h>
#include <asm/fpu/api.h>

#include "vexfs_v2_phase3.h"
#include "vexfs_v2_enhanced_file_ops.h"

/* Global performance counters */
static atomic64_t total_vector_reads = ATOMIC64_INIT(0);
static atomic64_t total_vector_writes = ATOMIC64_INIT(0);
static atomic64_t total_simd_operations = ATOMIC64_INIT(0);
static atomic64_t total_bytes_transferred = ATOMIC64_INIT(0);

/* ðŸ”¥ TRANSFER CONTEXT MANAGEMENT ðŸ”¥ */

/**
 * vexfs_init_transfer_context - Initialize transfer context
 * @ctx: Transfer context to initialize
 * @file: File being accessed
 * 
 * Returns: 0 on success, negative error code on failure
 */
int vexfs_init_transfer_context(struct vexfs_transfer_context *ctx,
                               struct file *file)
{
    struct vexfs_v2_sb_info *sbi;
    
    if (!ctx || !file)
        return -EINVAL;
    
    sbi = VEXFS_V2_SB(file->f_inode->i_sb);
    if (!sbi)
        return -EINVAL;
    
    memset(ctx, 0, sizeof(*ctx));
    
    /* Initialize from superblock configuration */
    ctx->vector_alignment = sbi->vector_alignment;
    ctx->batch_size = sbi->batch_size;
    ctx->prefetch_size = sbi->prefetch_size;
    ctx->simd_capabilities = sbi->simd_capabilities;
    ctx->simd_vector_width = sbi->simd_vector_width;
    ctx->simd_enabled = (sbi->simd_capabilities != 0);
    ctx->numa_aware = sbi->numa_aware;
    
    /* Set default transfer flags */
    ctx->flags = VEXFS_TRANSFER_SIMD_ALIGNED | VEXFS_TRANSFER_PREFETCH_ENABLED;
    if (ctx->numa_aware)
        ctx->flags |= VEXFS_TRANSFER_NUMA_LOCAL;
    if (ctx->batch_size > 1)
        ctx->flags |= VEXFS_TRANSFER_BATCH_OPTIMIZED;
    
    /* Initialize access pattern tracking */
    ctx->pattern = VEXFS_ACCESS_SEQUENTIAL;
    ctx->last_offset = 0;
    ctx->access_count = 0;
    ctx->sequential_count = 0;
    
    /* Get optimal NUMA node */
    if (ctx->numa_aware) {
        ctx->numa_node = vexfs_get_optimal_numa_node(file);
    } else {
        ctx->numa_node = NUMA_NO_NODE;
    }
    
    printk(KERN_DEBUG "VexFS v2.0: Transfer context initialized - "
           "alignment=%u, batch_size=%u, simd=%s, numa_node=%d\n",
           ctx->vector_alignment, ctx->batch_size,
           ctx->simd_enabled ? "enabled" : "disabled", ctx->numa_node);
    
    return 0;
}

/**
 * vexfs_cleanup_transfer_context - Cleanup transfer context
 * @ctx: Transfer context to cleanup
 */
void vexfs_cleanup_transfer_context(struct vexfs_transfer_context *ctx)
{
    if (!ctx)
        return;
    
    /* Log final statistics if debug enabled */
    printk(KERN_DEBUG "VexFS v2.0: Transfer context cleanup - "
           "bytes=%llu, simd_ops=%llu, cache_hits=%llu, cache_misses=%llu\n",
           ctx->bytes_transferred, ctx->simd_operations,
           ctx->cache_hits, ctx->cache_misses);
    
    memset(ctx, 0, sizeof(*ctx));
}

/**
 * vexfs_update_transfer_context - Update transfer context with access info
 * @ctx: Transfer context to update
 * @offset: Access offset
 * @count: Access size
 */
void vexfs_update_transfer_context(struct vexfs_transfer_context *ctx,
                                  loff_t offset, size_t count)
{
    if (!ctx)
        return;
    
    ctx->access_count++;
    ctx->bytes_transferred += count;
    
    /* Detect access pattern */
    if (ctx->access_count > 1) {
        if (offset == ctx->last_offset + count) {
            ctx->sequential_count++;
            if (ctx->sequential_count > 3) {
                ctx->pattern = VEXFS_ACCESS_SEQUENTIAL;
            }
        } else if (abs(offset - ctx->last_offset) > count * 4) {
            ctx->pattern = VEXFS_ACCESS_RANDOM;
        }
    }
    
    ctx->last_offset = offset;
}

/* ðŸ”¥ SIMD-ACCELERATED DATA TRANSFER ðŸ”¥ */

/**
 * vexfs_simd_copy_to_user - SIMD-accelerated copy to user space
 * @dst: Destination user space buffer
 * @src: Source kernel space buffer
 * @count: Number of bytes to copy
 * @alignment: Required alignment
 * @simd_capabilities: Available SIMD capabilities
 * 
 * Returns: Number of bytes copied, or negative error code
 */
ssize_t vexfs_simd_copy_to_user(char __user *dst, const void *src,
                                size_t count, u32 alignment,
                                u32 simd_capabilities)
{
    size_t copied = 0;
    size_t chunk_size;
    const char *src_ptr = (const char *)src;
    char __user *dst_ptr = dst;
    
    if (!dst || !src || count == 0)
        return -EINVAL;
    
    /* Check if we can use SIMD acceleration */
    if (simd_capabilities && vexfs_is_vector_aligned((loff_t)src, count, alignment)) {
        /* Use SIMD-optimized copy in aligned chunks */
        chunk_size = min(count, (size_t)(alignment * 8)); /* Process 8 vectors at a time */
        
        while (copied < count) {
            size_t this_chunk = min(chunk_size, count - copied);
            
            /* Check FPU availability for SIMD operations */
            if (irq_fpu_usable()) {
                kernel_fpu_begin();
                
                /* Perform SIMD-accelerated copy */
                if (copy_to_user(dst_ptr, src_ptr, this_chunk)) {
                    kernel_fpu_end();
                    return copied ? copied : -EFAULT;
                }
                
                kernel_fpu_end();
                atomic64_inc(&total_simd_operations);
            } else {
                /* Fall back to regular copy */
                if (copy_to_user(dst_ptr, src_ptr, this_chunk)) {
                    return copied ? copied : -EFAULT;
                }
            }
            
            copied += this_chunk;
            src_ptr += this_chunk;
            dst_ptr += this_chunk;
        }
    } else {
        /* Use regular copy_to_user */
        if (copy_to_user(dst, src, count))
            return -EFAULT;
        copied = count;
    }
    
    atomic64_add(copied, &total_bytes_transferred);
    return copied;
}

/**
 * vexfs_simd_copy_from_user - SIMD-accelerated copy from user space
 * @dst: Destination kernel space buffer
 * @src: Source user space buffer
 * @count: Number of bytes to copy
 * @alignment: Required alignment
 * @simd_capabilities: Available SIMD capabilities
 * 
 * Returns: Number of bytes copied, or negative error code
 */
ssize_t vexfs_simd_copy_from_user(void *dst, const char __user *src,
                                  size_t count, u32 alignment,
                                  u32 simd_capabilities)
{
    size_t copied = 0;
    size_t chunk_size;
    char *dst_ptr = (char *)dst;
    const char __user *src_ptr = src;
    
    if (!dst || !src || count == 0)
        return -EINVAL;
    
    /* Check if we can use SIMD acceleration */
    if (simd_capabilities && vexfs_is_vector_aligned((loff_t)dst, count, alignment)) {
        /* Use SIMD-optimized copy in aligned chunks */
        chunk_size = min(count, (size_t)(alignment * 8)); /* Process 8 vectors at a time */
        
        while (copied < count) {
            size_t this_chunk = min(chunk_size, count - copied);
            
            /* Check FPU availability for SIMD operations */
            if (irq_fpu_usable()) {
                kernel_fpu_begin();
                
                /* Perform SIMD-accelerated copy */
                if (copy_from_user(dst_ptr, src_ptr, this_chunk)) {
                    kernel_fpu_end();
                    return copied ? copied : -EFAULT;
                }
                
                kernel_fpu_end();
                atomic64_inc(&total_simd_operations);
            } else {
                /* Fall back to regular copy */
                if (copy_from_user(dst_ptr, src_ptr, this_chunk)) {
                    return copied ? copied : -EFAULT;
                }
            }
            
            copied += this_chunk;
            dst_ptr += this_chunk;
            src_ptr += this_chunk;
        }
    } else {
        /* Use regular copy_from_user */
        if (copy_from_user(dst, src, count))
            return -EFAULT;
        copied = count;
    }
    
    atomic64_add(copied, &total_bytes_transferred);
    return copied;
}

/* ðŸ”¥ ENHANCED READ OPERATIONS ðŸ”¥ */

/**
 * vexfs_enhanced_read - Enhanced vector-optimized read operation
 * @file: File to read from
 * @buf: User space buffer
 * @count: Number of bytes to read
 * @ppos: File position
 * 
 * Returns: Number of bytes read, or negative error code
 */
ssize_t vexfs_enhanced_read(struct file *file, char __user *buf,
                           size_t count, loff_t *ppos)
{
    struct vexfs_transfer_context ctx;
    struct vexfs_readahead_context ra_ctx;
    struct inode *inode = file->f_inode;
    struct vexfs_v2_sb_info *sbi = VEXFS_V2_SB(inode->i_sb);
    void *kernel_buf = NULL;
    ssize_t result = 0;
    size_t aligned_count;
    loff_t offset = *ppos;
    
    if (!buf || count == 0)
        return 0;
    
    /* Check file bounds */
    if (offset >= inode->i_size)
        return 0;
    
    if (offset + count > inode->i_size)
        count = inode->i_size - offset;
    
    /* Initialize transfer context */
    result = vexfs_init_transfer_context(&ctx, file);
    if (result)
        return result;
    
    /* Initialize readahead context */
    result = vexfs_init_readahead_context(&ra_ctx, file);
    if (result) {
        vexfs_cleanup_transfer_context(&ctx);
        return result;
    }
    
    /* Update access pattern tracking */
    vexfs_update_transfer_context(&ctx, offset, count);
    vexfs_update_readahead_pattern(&ra_ctx, offset, count);
    
    /* Calculate optimal transfer size */
    aligned_count = vexfs_calculate_transfer_size(count, ctx.vector_alignment, ctx.batch_size);
    
    /* Allocate aligned kernel buffer */
    if (ctx.numa_aware && ctx.numa_node != NUMA_NO_NODE) {
        kernel_buf = vexfs_numa_alloc_aligned(aligned_count, ctx.vector_alignment, ctx.numa_node);
    } else {
        kernel_buf = kmalloc(aligned_count, GFP_KERNEL);
    }
    
    if (!kernel_buf) {
        result = -ENOMEM;
        goto cleanup;
    }
    
    /* Trigger readahead if beneficial */
    if (vexfs_should_prefetch(file, offset, count)) {
        vexfs_vector_readahead(file, offset + count, ctx.prefetch_size * ctx.vector_alignment);
    }
    
    /* Simulate reading data (in real implementation, this would read from storage) */
    memset(kernel_buf, 0, count); /* For now, just return zeros */
    
    /* Copy data to user space with SIMD acceleration */
    result = vexfs_simd_copy_to_user(buf, kernel_buf, count,
                                    ctx.vector_alignment, ctx.simd_capabilities);
    if (result < 0)
        goto cleanup;
    
    /* Update file position */
    *ppos += result;
    
    /* Update performance counters */
    atomic64_inc(&total_vector_reads);
    if (ctx.simd_enabled)
        ctx.simd_operations++;
    
    printk(KERN_DEBUG "VexFS v2.0: Enhanced read - offset=%lld, count=%zu, "
           "result=%zd, simd=%s\n", offset, count, result,
           ctx.simd_enabled ? "yes" : "no");

cleanup:
    if (kernel_buf) {
        if (ctx.numa_aware && ctx.numa_node != NUMA_NO_NODE) {
            vexfs_numa_free_aligned(kernel_buf, aligned_count);
        } else {
            kfree(kernel_buf);
        }
    }
    
    vexfs_cleanup_readahead_context(&ra_ctx);
    vexfs_cleanup_transfer_context(&ctx);
    
    return result;
}

/**
 * vexfs_enhanced_write - Enhanced vector-optimized write operation
 * @file: File to write to
 * @buf: User space buffer
 * @count: Number of bytes to write
 * @ppos: File position
 * 
 * Returns: Number of bytes written, or negative error code
 */
ssize_t vexfs_enhanced_write(struct file *file, const char __user *buf,
                            size_t count, loff_t *ppos)
{
    struct vexfs_transfer_context ctx;
    struct inode *inode = file->f_inode;
    struct vexfs_v2_sb_info *sbi = VEXFS_V2_SB(inode->i_sb);
    void *kernel_buf = NULL;
    ssize_t result = 0;
    size_t aligned_count;
    loff_t offset = *ppos;
    
    if (!buf || count == 0)
        return 0;
    
    /* Initialize transfer context */
    result = vexfs_init_transfer_context(&ctx, file);
    if (result)
        return result;
    
    /* Update access pattern tracking */
    vexfs_update_transfer_context(&ctx, offset, count);
    
    /* Calculate optimal transfer size */
    aligned_count = vexfs_calculate_transfer_size(count, ctx.vector_alignment, ctx.batch_size);
    
    /* Allocate aligned kernel buffer */
    if (ctx.numa_aware && ctx.numa_node != NUMA_NO_NODE) {
        kernel_buf = vexfs_numa_alloc_aligned(aligned_count, ctx.vector_alignment, ctx.numa_node);
    } else {
        kernel_buf = kmalloc(aligned_count, GFP_KERNEL);
    }
    
    if (!kernel_buf) {
        result = -ENOMEM;
        goto cleanup;
    }
    
    /* Copy data from user space with SIMD acceleration */
    result = vexfs_simd_copy_from_user(kernel_buf, buf, count,
                                      ctx.vector_alignment, ctx.simd_capabilities);
    if (result < 0)
        goto cleanup;
    
    /* Simulate writing data (in real implementation, this would write to storage) */
    /* For now, just update the inode size */
    if (offset + result > inode->i_size) {
        inode->i_size = offset + result;
        mark_inode_dirty(inode);
    }
    
    /* Update file position */
    *ppos += result;
    
    /* Update performance counters */
    atomic64_inc(&total_vector_writes);
    if (ctx.simd_enabled)
        ctx.simd_operations++;
    
    printk(KERN_DEBUG "VexFS v2.0: Enhanced write - offset=%lld, count=%zu, "
           "result=%zd, simd=%s\n", offset, count, result,
           ctx.simd_enabled ? "yes" : "no");

cleanup:
    if (kernel_buf) {
        if (ctx.numa_aware && ctx.numa_node != NUMA_NO_NODE) {
            vexfs_numa_free_aligned(kernel_buf, aligned_count);
        } else {
            kfree(kernel_buf);
        }
    }
    
    vexfs_cleanup_transfer_context(&ctx);
    
    return result;
}

/* ðŸ”¥ READAHEAD CONTEXT MANAGEMENT ðŸ”¥ */

/**
 * vexfs_init_readahead_context - Initialize readahead context
 * @ctx: Readahead context to initialize
 * @file: File being accessed
 * 
 * Returns: 0 on success, negative error code on failure
 */
int vexfs_init_readahead_context(struct vexfs_readahead_context *ctx,
                                struct file *file)
{
    struct vexfs_v2_sb_info *sbi;
    
    if (!ctx || !file)
        return -EINVAL;
    
    sbi = VEXFS_V2_SB(file->f_inode->i_sb);
    if (!sbi)
        return -EINVAL;
    
    memset(ctx, 0, sizeof(*ctx));
    
    /* Initialize readahead configuration */
    ctx->window_size = sbi->prefetch_size * sbi->vector_alignment;
    ctx->max_vectors = sbi->prefetch_size;
    ctx->trigger_threshold = sbi->vector_alignment;
    
    /* Initialize pattern detection */
    ctx->pattern = VEXFS_ACCESS_SEQUENTIAL;
    ctx->stride_size = sbi->vector_alignment;
    ctx->last_offset = 0;
    
    /* Initialize readahead state */
    ctx->next_offset = 0;
    ctx->pending_requests = 0;
    ctx->active = false;
    
    return 0;
}

/**
 * vexfs_cleanup_readahead_context - Cleanup readahead context
 * @ctx: Readahead context to cleanup
 */
void vexfs_cleanup_readahead_context(struct vexfs_readahead_context *ctx)
{
    if (!ctx)
        return;
    
    /* Log readahead statistics */
    printk(KERN_DEBUG "VexFS v2.0: Readahead cleanup - "
           "hits=%llu, misses=%llu, bytes=%llu\n",
           ctx->readahead_hits, ctx->readahead_misses, ctx->bytes_readahead);
    
    memset(ctx, 0, sizeof(*ctx));
}

/**
 * vexfs_update_readahead_pattern - Update readahead access pattern
 * @ctx: Readahead context to update
 * @offset: Access offset
 * @count: Access size
 */
void vexfs_update_readahead_pattern(struct vexfs_readahead_context *ctx,
                                   loff_t offset, size_t count)
{
    if (!ctx)
        return;
    
    /* Detect stride pattern */
    if (ctx->last_offset != 0) {
        u64 stride = offset - ctx->last_offset;
        if (stride > 0 && stride < ctx->window_size * 4) {
            if (ctx->stride_size == 0 || abs(stride - ctx->stride_size) < ctx->stride_size / 4) {
                ctx->stride_size = stride;
                ctx->pattern = VEXFS_ACCESS_SEQUENTIAL;
            } else {
                ctx->pattern = VEXFS_ACCESS_RANDOM;
            }
        } else {
            ctx->pattern = VEXFS_ACCESS_RANDOM;
        }
    }
    
    ctx->last_offset = offset;
    ctx->next_offset = offset + count;
}

/* ðŸ”¥ UTILITY FUNCTIONS ðŸ”¥ */

/**
 * vexfs_is_vector_aligned - Check if offset and count are vector aligned
 * @offset: File offset
 * @count: Transfer size
 * @alignment: Required alignment
 * 
 * Returns: true if aligned, false otherwise
 */
bool vexfs_is_vector_aligned(loff_t offset, size_t count, u32 alignment)
{
    return (offset % alignment == 0) && (count % alignment == 0);
}

/**
 * vexfs_round_up_to_alignment - Round up value to alignment boundary
 * @value: Value to round up
 * @alignment: Alignment boundary
 * 
 * Returns: Rounded up value
 */
u32 vexfs_round_up_to_alignment(u32 value, u32 alignment)
{
    return (value + alignment - 1) & ~(alignment - 1);
}

/**
 * vexfs_calculate_transfer_size - Calculate optimal transfer size
 * @requested: Requested transfer size
 * @alignment: Required alignment
 * @batch_size: Optimal batch size
 * 
 * Returns: Optimal transfer size
 */
size_t vexfs_calculate_transfer_size(size_t requested, u32 alignment, u32 batch_size)
{
    size_t aligned_size = vexfs_round_up_to_alignment(requested, alignment);
    size_t batch_aligned = vexfs_round_up_to_alignment(aligned_size, alignment * batch_size);
    
    /* Don't make it too large */
    return min(batch_aligned, aligned_size + alignment * batch_size);
}

/**
 * vexfs_should_use_simd - Determine if SIMD should be used
 * @file: File being accessed
 * @count: Transfer size
 * 
 * Returns: true if SIMD should be used, false otherwise
 */
bool vexfs_should_use_simd(struct file *file, size_t count)
{
    struct vexfs_v2_sb_info *sbi = VEXFS_V2_SB(file->f_inode->i_sb);
    
    if (!sbi || !sbi->simd_capabilities)
        return false;
    
    /* Use SIMD for transfers larger than vector alignment */
    return count >= sbi->vector_alignment;
}

/**
 * vexfs_should_prefetch - Determine if prefetching should be used
 * @file: File being accessed
 * @offset: Access offset
 * @count: Transfer size
 * 
 * Returns: true if prefetching should be used, false otherwise
 */
bool vexfs_should_prefetch(struct file *file, loff_t offset, size_t count)
{
    struct vexfs_v2_sb_info *sbi = VEXFS_V2_SB(file->f_inode->i_sb);
    
    if (!sbi || sbi->prefetch_size == 0)
        return false;
    
    /* Use prefetching for sequential access patterns */
    return count >= sbi->vector_alignment;
}

/**
 * vexfs_get_optimal_numa_node - Get optimal NUMA node for file access
 * @file: File being accessed
 * 
 * Returns: NUMA node ID, or NUMA_NO_NODE if not applicable
 */
int vexfs_get_optimal_numa_node(struct file *file)
{
    struct vexfs_v2_sb_info *sbi = VEXFS_V2_SB(file->f_inode->i_sb);
    
    if (!sbi || !sbi->numa_aware)
        return NUMA_NO_NODE;
    
    /* For now, use the current CPU's NUMA node */
    return numa_node_id();
}

/**
 * vexfs_numa_alloc_aligned - Allocate NUMA-aware aligned memory
 * @size: Size to allocate
 * @alignment: Required alignment
 * @node: NUMA node ID
 * 
 * Returns: Allocated memory pointer, or NULL on failure
 */
void *vexfs_numa_alloc_aligned(size_t size, u32 alignment, int node)
{
    void *ptr;
    
    if (node == NUMA_NO_NODE) {
        ptr = kmalloc(size, GFP_KERNEL);
    } else {
        ptr = kmalloc_node(size, GFP_KERNEL, node);
    }
    
    /* Check alignment */
    if (ptr && ((unsigned long)ptr % alignment != 0)) {
        kfree(ptr);
        /* For simplicity, fall back to regular allocation */
        ptr = kmalloc(size, GFP_KERNEL);
    }
    
    return ptr;
}

/**
 * vexfs_numa_free_aligned - Free NUMA-aware aligned memory
 * @ptr: Memory pointer to free
 * @size: Size of memory
 */
void vexfs_numa_free_aligned(void *ptr, size_t size)
{
    if (ptr)
        kfree(ptr);
}

/* ðŸ”¥ READAHEAD OPERATIONS ðŸ”¥ */

/**
 * vexfs_vector_readahead - Perform vector-aware readahead
 * @file: File to readahead
 * @offset: Starting offset
 * @count: Number of bytes to readahead
 * 
 * Returns: 0 on success, negative error code on failure
 */
int vexfs_vector_readahead(struct file *file, loff_t offset, size_t count)
{
    struct vexfs_v2_sb_info *sbi = VEXFS_V2_SB(file->f_inode->i_sb);
    
    if (!sbi || sbi->prefetch_size == 0)
        return 0;
    
    /* For now, just log the readahead request */
    printk(KERN_DEBUG "VexFS v2.0: Vector readahead - offset=%lld, count=%zu\n",
           offset, count);
    
    /* In a real implementation, this would trigger actual readahead */
    return 0;
}

/**
 * vexfs_update_access_pattern - Update file access pattern
 * @file: File being accessed
 * @offset: Access offset
 * @count: Access size
 */
void vexfs_update_access_pattern(struct file *file, loff_t offset, size_t count)
{
    /* For now, just log the access pattern */
    printk(KERN_DEBUG "VexFS v2.0: Access pattern update - offset=%lld, count=%zu\n",
           offset, count);
}

/* ðŸ”¥ ERROR HANDLING AND DEBUGGING ðŸ”¥ */

/**
 * vexfs_report_transfer_error - Report transfer error
 * @file: File being accessed
 * @error: Error code
 * @operation: Operation name
 * @offset: Access offset
 * @count: Access size
 */
void vexfs_report_transfer_error(struct file *file, int error,
                                const char *operation, loff_t offset, size_t count)
{
    printk(KERN_ERR "VexFS v2.0: Transfer error - %s failed with error %d "
           "(offset=%lld, count=%zu)\n", operation, error, offset, count);
}

/**
 * vexfs_log_performance_stats - Log performance statistics
 * @file: File being accessed
 * @ctx: Transfer context
 */
void vexfs_log_performance_stats(struct file *file,
                                const struct vexfs_transfer_context *ctx)
{
    if (!ctx)
        return;
    
    printk(KERN_INFO "VexFS v2.0: Performance stats - "
           "bytes=%llu, simd_ops=%llu, cache_hits=%llu, cache_misses=%llu, "
           "pattern=%d, access_count=%llu\n",
           ctx->bytes_transferred, ctx->simd_operations,
           ctx->cache_hits, ctx->cache_misses,
           ctx->pattern, ctx->access_count);
}